{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from vgc.datatypes.Objects import PkmTeam, Pkm, GameState, Weather\n",
    "from vgc.engine.PkmBattleEnv import PkmBattleEnv\n",
    "from vgc.util.generator.PkmTeamGenerators import RandomTeamGenerator\n",
    "\n",
    "from vgc.datatypes.Constants import TYPE_CHART_MULTIPLIER, MAX_HIT_POINTS, MOVE_MAX_PP, DEFAULT_TEAM_SIZE\n",
    "from vgc.datatypes.Objects import PkmMove, Pkm, PkmTeam, GameState, Weather\n",
    "from vgc.datatypes.Types import PkmStat, PkmType, WeatherCondition, \\\n",
    "    N_TYPES, N_STATUS, N_STATS, N_ENTRY_HAZARD, N_WEATHER, PkmStatus, PkmEntryHazard\n",
    "\n",
    "import math\n",
    "import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dmg to and from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_turns_to_faint_list(team_1_game_state, team_2_game_state, max_turns_to_faint_value):\n",
    "    '''\n",
    "    put in zero for the fainted\n",
    "        handle the check for fainted in HP\n",
    "    assume everything is revealed\n",
    "        handle masking elsewhere\n",
    "    '''\n",
    "\n",
    "    # Get weather condition\n",
    "    weather = team_1_game_state.weather.condition\n",
    "\n",
    "    # Get my Pokémon team\n",
    "    team_1 = team_1_game_state.teams[0]\n",
    "    team_1_pkm_list = [team_1.active] + team_1.party\n",
    "\n",
    "    # Get opponent's team\n",
    "    team_2 = team_2_game_state.teams[0]\n",
    "    team_2_pkm_list = [team_2.active] + team_2.party\n",
    "\n",
    "    # Iterate over all my Pokémon and their moves to find the most damaging move\n",
    "    best_damage_list = []\n",
    "    turns_to_faint_list = []\n",
    "    hp_list = []\n",
    "\n",
    "    for team_1_pkm_index, team_1_pkm in enumerate(team_1_pkm_list):\n",
    "        # Initialize variables for the best move and its damage\n",
    "        best_damage = -np.inf\n",
    "\n",
    "        for team_2_pkm_index, team_2_pkm in enumerate(team_2_pkm_list):\n",
    "            if team_1_pkm_index == 0:\n",
    "                team_1_attack_stage = team_1.stage[PkmStat.ATTACK]\n",
    "            else:\n",
    "                team_1_attack_stage = 0\n",
    "            \n",
    "            if team_2_pkm_index == 0:\n",
    "                team_2_defense_stage = team_2.stage[PkmStat.DEFENSE]\n",
    "            else:\n",
    "                team_2_defense_stage = 0\n",
    "\n",
    "            for move_index, move in enumerate(team_1_pkm.moves):\n",
    "                \n",
    "                damage = estimate_damage(move.type, team_1_pkm.type, move.power, team_2_pkm.type, team_1_attack_stage,\n",
    "                                            team_2_defense_stage, weather)\n",
    "\n",
    "                # Check if the current move has higher damage than the previous best move\n",
    "                if damage > best_damage:\n",
    "                    best_damage = damage\n",
    "\n",
    "            # get best dmg for each pokemon\n",
    "            best_damage_list.append(best_damage)\n",
    "            hp_list.append(team_2_pkm.hp)\n",
    "\n",
    "            if best_damage > 0.:\n",
    "                turns_to_faint = math.ceil(team_2_pkm.hp / best_damage)\n",
    "            else:\n",
    "                turns_to_faint = max_turns_to_faint_value\n",
    "\n",
    "            turns_to_faint_list.append(turns_to_faint)\n",
    "\n",
    "    print(turns_to_faint_list)\n",
    "    print(best_damage_list)\n",
    "    print(hp_list)\n",
    "\n",
    "    return turns_to_faint_list\n",
    "\n",
    "\n",
    "def estimate_damage(move_type: PkmType, pkm_type: PkmType, move_power: float, opp_pkm_type: PkmType,\n",
    "                    attack_stage: int, defense_stage: int, weather: WeatherCondition) -> float:\n",
    "        '''\n",
    "        from updated repo\n",
    "        '''\n",
    "        stab = 1.5 if move_type == pkm_type else 1.\n",
    "        if (move_type == PkmType.WATER and weather == WeatherCondition.RAIN) or (\n",
    "                move_type == PkmType.FIRE and weather == WeatherCondition.SUNNY):\n",
    "            weather = 1.5\n",
    "        elif (move_type == PkmType.WATER and weather == WeatherCondition.SUNNY) or (\n",
    "                move_type == PkmType.FIRE and weather == WeatherCondition.RAIN):\n",
    "            weather = .5\n",
    "        else:\n",
    "            weather = 1.\n",
    "        stage_level = attack_stage - defense_stage\n",
    "        stage = (stage_level + 2.) / 2 if stage_level >= 0. else 2. / (np.abs(stage_level) + 2.)\n",
    "        damage = TYPE_CHART_MULTIPLIER[move_type][opp_pkm_type] * stab * weather * stage * move_power\n",
    "\n",
    "        #print(damage, move_type, pkm_type, move_power, opp_pkm_type, attack_stage, defense_stage, weather)\n",
    "        return damage\n",
    "\n",
    "\n",
    "def save_object_as_pkl(object_to_save, save_tag):\n",
    "    '''\n",
    "    Save object a pickle file\n",
    "    '''\n",
    "    with open(f'{save_tag}.pickle', 'wb') as handle:\n",
    "        pickle.dump(object_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n",
      "[153.0, 210.0, 130.5, 348.0]\n",
      "[120.0, 156.0, 120.0, 156.0]\n",
      "[1, 1, 1, 1]\n",
      "[138.0, 348.0, 207.0, 207.0]\n",
      "[120.0, 156.0, 120.0, 156.0]\n",
      "[1, 1, 2, 1]\n",
      "[369.0, 369.0, 207.0, 207.0]\n",
      "[264.0, 192.0, 264.0, 192.0]\n",
      "[1, 1, 1, 1]\n",
      "[306.0, 306.0, 276.0, 276.0]\n",
      "[156.0, 264.0, 156.0, 264.0]\n",
      "[1, 1, 1, 1]\n",
      "[276.0, 276.0, 198.0, 198.0]\n",
      "[156.0, 192.0, 156.0, 192.0]\n",
      "[2, 2, 2, 1]\n",
      "[246.0, 246.0, 369.0, 369.0]\n",
      "[372.0, 336.0, 372.0, 336.0]\n",
      "[1, 1, 2, 1]\n",
      "[261.0, 261.0, 138.0, 138.0]\n",
      "[192.0, 120.0, 192.0, 120.0]\n",
      "[1, 1, 1, 1]\n",
      "[315.0, 315.0, 210.0, 306.0]\n",
      "[156.0, 192.0, 156.0, 192.0]\n",
      "[2, 2, 1, 2]\n",
      "[102.0, 348.0, 276.0, 276.0]\n",
      "[192.0, 408.0, 192.0, 408.0]\n",
      "[2, 2, 1, 1]\n",
      "[102.0, 174.0, 204.0, 204.0]\n",
      "[192.0, 192.0, 192.0, 192.0]\n",
      "Time to run 0.009 seconds\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_iters = 5\n",
    "max_state_value = 1000000\n",
    "time_start = time.time()\n",
    "\n",
    "raw_stats_dict = {}\n",
    "turns_to_faint_dict = {}\n",
    "\n",
    "team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "for i in range(test_iters):\n",
    "    agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "    opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "    env = PkmBattleEnv((agent_team, opp_team), encode=(False, False)) \n",
    "\n",
    "    game_state, info = env.reset()\n",
    "\n",
    "    team_1_faint_list = get_turns_to_faint_list(game_state[0], game_state[1], max_state_value)\n",
    "    team_2_faint_list = get_turns_to_faint_list(game_state[1], game_state[0], max_state_value)\n",
    "\n",
    "    state_tuple = tuple(team_1_faint_list + team_2_faint_list)\n",
    "\n",
    "    if state_tuple in raw_stats_dict:\n",
    "        raw_stats_dict[state_tuple] += 1\n",
    "    else:\n",
    "        raw_stats_dict[state_tuple] = 1\n",
    "\n",
    "    for turns in team_1_faint_list + team_2_faint_list:\n",
    "        if turns in turns_to_faint_dict:\n",
    "            turns_to_faint_dict[turns] += 1\n",
    "        else:\n",
    "            turns_to_faint_dict[turns] = 1\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"Time to run {time_end - time_start:.3f} seconds\")\n",
    "\n",
    "print(len(raw_stats_dict))\n",
    "print(len(turns_to_faint_dict))\n",
    "\n",
    "save_object_as_pkl(raw_stats_dict, f'turns_to_faint_state_dict_{int(time_start)}')\n",
    "save_object_as_pkl(turns_to_faint_dict, f'turns_to_faint_count_dict_{int(time_start)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 33, 2: 6, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "print(turns_to_faint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_stats_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_turns_to_faint_array(hp_list, best_dmg_list, max_state_value):\n",
    "#     '''\n",
    "#     hp list is indexed 0 to 3 with\n",
    "#     0 agent active\n",
    "#     1 agent party best\n",
    "#     2 opp active\n",
    "#     3 opp party best\n",
    "\n",
    "#     best_dmg_list is indexed 0 to 7\n",
    "#     0 agent active dmg to opp active\n",
    "#     1 agent active dmg to opp party best\n",
    "#     2 agent party best dmg to opp active\n",
    "#     3 agent party best dmg to opp party best\n",
    "#     4 opp active dmg to agent active\n",
    "#     5 opp active dmg to agent party best\n",
    "#     6 opp party best dmg to agent active\n",
    "#     7 opp party best dmg to agent party best\n",
    "\n",
    "#     '''\n",
    "#     turns_to_faint_array = np.ones((8, ), dtype=np.float32) * -1.\n",
    "\n",
    "#     agent_active_hp_index = 0\n",
    "#     agent_party_best_hp_index = 1\n",
    "#     opp_active_hp_index = 2\n",
    "#     opp_party_best_hp_index = 3\n",
    "\n",
    "#     agent_active_dmg_to_opp_active_index = 0\n",
    "#     agent_active_dmg_to_opp_party_best_index = 1\n",
    "#     agent_party_best_dmg_to_opp_active_index = 2\n",
    "#     agent_party_best_dmg_to_opp_party_best_index = 3\n",
    "#     opp_active_dmg_to_agent_active_index = 4\n",
    "#     opp_active_dmg_to_agent_party_best_index = 5\n",
    "#     opp_party_best_dmg_to_agent_active_index = 6\n",
    "#     opp_party_best_dmg_to_agent_party_best_index = 7\n",
    "\n",
    "#     turns_to_faint_array[0] = get_dmg_turns_to_faint(hp_list[agent_active_hp_index], \n",
    "#         best_dmg_list[agent_active_dmg_to_opp_active_index], max_state_value)\n",
    "#     turns_to_faint_array[1] = get_dmg_turns_to_faint(hp_list[agent_active_hp_index],\n",
    "#         best_dmg_list[agent_active_dmg_to_opp_party_best_index], max_state_value)\n",
    "\n",
    "#     turns_to_faint_array[2] = get_dmg_turns_to_faint(hp_list[agent_party_best_hp_index],\n",
    "#         best_dmg_list[agent_party_best_dmg_to_opp_active_index], max_state_value)\n",
    "#     turns_to_faint_array[3] = get_dmg_turns_to_faint(hp_list[agent_party_best_hp_index],\n",
    "#         best_dmg_list[agent_party_best_dmg_to_opp_party_best_index], max_state_value)\n",
    "\n",
    "#     turns_to_faint_array[4] = get_dmg_turns_to_faint(hp_list[opp_active_hp_index],\n",
    "#         best_dmg_list[opp_active_dmg_to_agent_active_index], max_state_value)\n",
    "#     turns_to_faint_array[5] = get_dmg_turns_to_faint(hp_list[opp_active_hp_index],\n",
    "#         best_dmg_list[opp_active_dmg_to_agent_party_best_index], max_state_value)\n",
    "\n",
    "#     turns_to_faint_array[6] = get_dmg_turns_to_faint(hp_list[opp_party_best_hp_index],\n",
    "#         best_dmg_list[opp_party_best_dmg_to_agent_active_index], max_state_value)\n",
    "#     turns_to_faint_array[7] = get_dmg_turns_to_faint(hp_list[opp_party_best_hp_index],\n",
    "#         best_dmg_list[opp_party_best_dmg_to_agent_party_best_index], max_state_value)\n",
    "\n",
    "    \n",
    "#     turns_to_faint_array = np.ceil(turns_to_faint_array)\n",
    "#     turns_to_faint_array = turns_to_faint_array.astype(np.int32)\n",
    "#     turns_to_faint_array = turns_to_faint_array.clip(-1, max_state_value)\n",
    "\n",
    "#     return turns_to_faint_array\n",
    "\n",
    "# turns_to_faint_array = np.ones((8, ), dtype=np.int32) * -1\n",
    "# turns_to_faint_array\n",
    "# np.ones((8, ), dtype=np.float32) \n",
    "# a = np.array([1.1, 2.0, 3, 4.00001, 5.00000, 5.99999, 7, 8])  \n",
    "# b = np.ceil(a)\n",
    "# b = b.astype(int)\n",
    "# c = tuple(b)\n",
    "# c\n",
    "# import math\n",
    "# type(math.ceil(2/1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing MC Env Learning in 2 v 2 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "working\n",
    "    finish up the loop function\n",
    "        smoke test it\n",
    "        see if saved action dict does anything eval wise. does it even trigger?\n",
    "            are results better\n",
    "    run tests on dict action space size\n",
    "    probably want to expand functions to at least what opp dmg is to party\n",
    "    check the functions work\n",
    "\n",
    "to do important:\n",
    "    can't swap if swp is identical or will be stuck in a swap loop\n",
    "    maybe add memory saying if swapped at 2 v 2 (or other levels as well)\n",
    "\n",
    "to do improvements:\n",
    "    know the opp dmg to current team. can add those states\n",
    "        DONE YES confirm this. might be a weird trick with revealed states\n",
    "    maybe find how many states are possible with raw values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from vgc.datatypes.Objects import PkmTeam, Pkm, GameState, Weather\n",
    "from vgc.engine.PkmBattleEnv import PkmBattleEnv\n",
    "from vgc.util.generator.PkmTeamGenerators import RandomTeamGenerator\n",
    "\n",
    "from vgc.datatypes.Constants import TYPE_CHART_MULTIPLIER, MAX_HIT_POINTS, MOVE_MAX_PP, DEFAULT_TEAM_SIZE\n",
    "from vgc.datatypes.Objects import PkmMove, Pkm, PkmTeam, GameState, Weather\n",
    "from vgc.datatypes.Types import PkmStat, PkmType, WeatherCondition, \\\n",
    "    N_TYPES, N_STATUS, N_STATS, N_ENTRY_HAZARD, N_WEATHER, PkmStatus, PkmEntryHazard\n",
    "\n",
    "\n",
    "import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nneed to turn the states into the dict\\n    TEST want to store counts and running mean of the reward\\n\\nset up the evaluation loop for this agent vs. the base always attack agent\\n\\nTEST need to test how many battles can get through in how much time\\n\\nTEST saving the action dict\\n\\nTEST need to select the initial action\\nTEST need to then to attack later\\nTEST need to convert the attack action into the best attack\\nTEST need to convert the swap action into a swap\\nTEST need to store the result of the battle\\nTEST convert outcome into a rewards\\n\\nLater\\n\\nfunctionalize the build dict loop\\nfunctionalize the eval\\ncan run the dict and eval in python files\\ncan parallelize the dict building\\n\\nmaybe store all states in a list then to the dict\\n    works I think as long as all actions past that point are attacks\\n    could then combine that later with a swap at that point maybe?\\n        idk maybe not... could be the 2nd swap and then things are necessarily clear\\n            ie initial pkm has been revealed and may have taken dmg (though maybe that doesn't matter)\\n\\npossibly store the state dict attack action for non first actions as well\\n\\n            \\nneed to add the hiding part\\n    i guess it's more like some states don't know opp dmg to current pkm and sometimes do\\n\\n    maybe parallelize if any of this works\\n\\n\\ncan I get the best dmg from both teams even if the pkm stuff is hidden and not revealed?\\n    probably yes since passing in the team specific state\\n\\ncan check to see how accurate the attack function is\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "grab the value from the lookup dict and see if can be used\n",
    "\n",
    "DRY for the two loops\n",
    "\n",
    "review flow of all code\n",
    "\n",
    "reveiw and test all parts of the code\n",
    "\n",
    "can this be generalized more?\n",
    "    ie not just for the first move when eval but any move?\n",
    "    idk, probably not for now\n",
    "\n",
    "due to the variability in outcomes I think I need a ton of results to tell if swap is better or not\n",
    "    like 1000 for each state and since like 500,000 states that is 500,000,000 battles\n",
    "\n",
    "TEST need to turn the states into the dict\n",
    "    TEST want to store counts and running mean of the reward\n",
    "\n",
    "TEST set up the evaluation loop for this agent vs. the base always attack agent\n",
    "\n",
    "TEST need to test how many battles can get through in how much time\n",
    "\n",
    "TEST saving the action dict\n",
    "\n",
    "TEST need to select the initial action\n",
    "TEST need to then to attack later\n",
    "TEST need to convert the attack action into the best attack\n",
    "TEST need to convert the swap action into a swap\n",
    "TEST need to store the result of the battle\n",
    "TEST convert outcome into a rewards\n",
    "\n",
    "Later\n",
    "\n",
    "functionalize the build dict loop\n",
    "functionalize the eval\n",
    "can run the dict and eval in python files\n",
    "can parallelize the dict building\n",
    "\n",
    "maybe store all states in a list then to the dict\n",
    "    works I think as long as all actions past that point are attacks\n",
    "    could then combine that later with a swap at that point maybe?\n",
    "        idk maybe not... could be the 2nd swap and then things are necessarily clear\n",
    "            ie initial pkm has been revealed and may have taken dmg (though maybe that doesn't matter)\n",
    "\n",
    "possibly store the state dict attack action for non first actions as well\n",
    "\n",
    "            \n",
    "need to add the hiding part\n",
    "    i guess it's more like some states don't know opp dmg to current pkm and sometimes do\n",
    "\n",
    "    maybe parallelize if any of this works\n",
    "\n",
    "\n",
    "can I get the best dmg from both teams even if the pkm stuff is hidden and not revealed?\n",
    "    probably yes since passing in the team specific state\n",
    "\n",
    "can check to see how accurate the attack function is\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_agent_action_into_env_action(action, agent_game_state):\n",
    "        '''\n",
    "        Action values are\n",
    "        0: select best move\n",
    "        1: switch to first pkm\n",
    "        2: switch to second pkm\n",
    "\n",
    "        Env actions are\n",
    "        0 to 3: action of active pokm\n",
    "        4: switch to first pkm\n",
    "        5: switch to second pkm\n",
    "        '''\n",
    "        # always get best move and action dmg list\n",
    "        best_active_action, best_damage_list = get_best_active_damage_action(agent_game_state)\n",
    "\n",
    "        if action == 0:\n",
    "            # get best dmg action\n",
    "            action = best_active_action\n",
    "        else:\n",
    "            # switch to first or second pkm if alive\n",
    "            if action == 1 or action == 2:\n",
    "                pkm = agent_game_state.teams[0].party[action-1]\n",
    "                if pkm.fainted() or pkm.hp <= 0.0:\n",
    "                    action = best_active_action\n",
    "                else:\n",
    "                    action = action + 3\n",
    "            else:\n",
    "                action = best_active_action\n",
    "\n",
    "        return action, best_damage_list\n",
    "\n",
    "\n",
    "def get_best_active_damage_action(g: GameState):\n",
    "    '''\n",
    "    '''\n",
    "    # Get weather condition\n",
    "    weather = g.weather.condition\n",
    "\n",
    "    # Get my Pokémon team\n",
    "    my_team = g.teams[0]\n",
    "    my_pkms = [my_team.active] + my_team.party\n",
    "\n",
    "    # Get opponent's team\n",
    "    opp_team = g.teams[1]\n",
    "    opp_active = opp_team.active\n",
    "\n",
    "    opp_active_type = opp_active.type\n",
    "    opp_defense_stage = opp_team.stage[PkmStat.DEFENSE]\n",
    "\n",
    "    # Iterate over all my Pokémon and their moves to find the most damaging move\n",
    "    best_dmg_list = []\n",
    "    best_move_list = []\n",
    "\n",
    "    for i, pkm in enumerate(my_pkms):\n",
    "        # Initialize variables for the best move and its damage\n",
    "        best_damage = -np.inf\n",
    "        best_move_id = -1\n",
    "\n",
    "        if i == 0:\n",
    "            my_attack_stage = my_team.stage[PkmStat.ATTACK]\n",
    "        else:\n",
    "            my_attack_stage = 0\n",
    "\n",
    "        for j, move in enumerate(pkm.moves):\n",
    "            \n",
    "            damage = estimate_damage(move.type, pkm.type, move.power, opp_active_type, my_attack_stage,\n",
    "                                        opp_defense_stage, weather)\n",
    "            \n",
    "            # Check if the current move has higher damage than the previous best move\n",
    "            if damage > best_damage:\n",
    "                best_move_id = j + i * 4 # think for 2024 j is 0 to 3 for each\n",
    "                best_damage = damage\n",
    "\n",
    "        # get best move and dmg for each pokemon\n",
    "        best_dmg_list.append(best_damage)\n",
    "        best_move_list.append(best_move_id)\n",
    "\n",
    "    active_pkm_best_move_id = best_move_list[0]\n",
    "\n",
    "    if active_pkm_best_move_id < 0 or active_pkm_best_move_id > 3:\n",
    "        print(f\"Error: best move id { active_pkm_best_move_id } not in expected range\")\n",
    "        active_pkm_best_move_id = 0\n",
    "\n",
    "    return active_pkm_best_move_id, best_dmg_list\n",
    "\n",
    "\n",
    "def estimate_damage(move_type: PkmType, pkm_type: PkmType, move_power: float, opp_pkm_type: PkmType,\n",
    "                    attack_stage: int, defense_stage: int, weather: WeatherCondition) -> float:\n",
    "        '''\n",
    "        Not from original code. from updated repo\n",
    "        '''\n",
    "        stab = 1.5 if move_type == pkm_type else 1.\n",
    "        if (move_type == PkmType.WATER and weather == WeatherCondition.RAIN) or (\n",
    "                move_type == PkmType.FIRE and weather == WeatherCondition.SUNNY):\n",
    "            weather = 1.5\n",
    "        elif (move_type == PkmType.WATER and weather == WeatherCondition.SUNNY) or (\n",
    "                move_type == PkmType.FIRE and weather == WeatherCondition.RAIN):\n",
    "            weather = .5\n",
    "        else:\n",
    "            weather = 1.\n",
    "        stage_level = attack_stage - defense_stage\n",
    "        stage = (stage_level + 2.) / 2 if stage_level >= 0. else 2. / (np.abs(stage_level) + 2.)\n",
    "        damage = TYPE_CHART_MULTIPLIER[move_type][opp_pkm_type] * stab * weather * stage * move_power\n",
    "\n",
    "        #print(damage, move_type, pkm_type, move_power, opp_pkm_type, attack_stage, defense_stage, weather)\n",
    "        return damage\n",
    "\n",
    "\n",
    "def save_object_as_pkl(object_to_save, save_tag):\n",
    "    '''\n",
    "    Save object a pickle file\n",
    "    '''\n",
    "    with open(f'{save_tag}.pickle', 'wb') as handle:\n",
    "        pickle.dump(object_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# make a dict that has keys for 0 to 100 and values for the action dict\n",
    "def make_lookup_dict():\n",
    "    lookup_dict = {}\n",
    "    for i in range(100):\n",
    "        if i <= 40:\n",
    "            lookup_value = i // 5\n",
    "        else:\n",
    "            lookup_value = 4 + i // 10\n",
    "        lookup_dict[i] = lookup_value\n",
    "    return lookup_dict\n",
    "\n",
    "# lookup_dict = make_lookup_dict()\n",
    "# pprint.pprint(lookup_dict)\n",
    "\n",
    "def get_win_loss_reward(terminated, winner, player_index):\n",
    "    '''\n",
    "    Does a reward for winning or losing\n",
    "    winner is -1 unless a winner has been picked\n",
    "    '''\n",
    "    reward = 0.\n",
    "    if terminated:\n",
    "\n",
    "        if winner == 0 or winner == 1:\n",
    "            if winner == player_index:\n",
    "                reward = 1.\n",
    "            else:\n",
    "                reward = -1.\n",
    "        #print(f\"reward {reward} | terminated {terminated} | winner {self.env.winner} | player_index {player_index}|\")\n",
    "    return reward\n",
    "\n",
    "def get_running_mean(old_mean, old_count, new_value):\n",
    "    '''\n",
    "    '''\n",
    "    new_mean = (old_mean * old_count + new_value) / (old_count + 1)\n",
    "    \n",
    "    return new_mean\n",
    "\n",
    "\n",
    "def add_results_to_action_dict(action_dict, state_key, agent_first_move, win_int):\n",
    "    '''\n",
    "    '''\n",
    "    count_key = \"count\"\n",
    "    sum_wins_key = \"sum_wins\"\n",
    "\n",
    "    if state_key in action_dict:\n",
    "        if agent_first_move in action_dict[state_key]:\n",
    "            action_dict[state_key][agent_first_move][sum_wins_key] += win_int\n",
    "            action_dict[state_key][agent_first_move][count_key] += 1\n",
    "        else:\n",
    "            action_dict[state_key][agent_first_move] = {}\n",
    "            action_dict[state_key][agent_first_move][sum_wins_key] = win_int\n",
    "            action_dict[state_key][agent_first_move][count_key] = 1\n",
    "    else:\n",
    "        action_dict[state_key] = {}\n",
    "        action_dict[state_key][agent_first_move] = {}\n",
    "        action_dict[state_key][agent_first_move][sum_wins_key] = win_int\n",
    "        action_dict[state_key][agent_first_move][count_key] = 1\n",
    "\n",
    "def add_action_to_pkm_env_action_dict(env_action, my_dict, team_key):\n",
    "    if env_action in my_dict[team_key]:\n",
    "        my_dict[team_key][env_action] += 1\n",
    "    else:\n",
    "        my_dict[team_key][env_action] = 1\n",
    "\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "\n",
    "# def add_results_to_action_dict(action_dict, state_key, agent_first_move, agent_reward):\n",
    "#     '''\n",
    "#     '''\n",
    "#     if state_key in action_dict:\n",
    "#         if agent_first_move in action_dict[state_key]:\n",
    "#             action_dict[state_key][agent_first_move][\"avg_reward\"] = get_running_mean(action_dict[state_key][agent_first_move][\"avg_reward\"],\n",
    "#                                                                                   action_dict[state_key][agent_first_move][\"count\"], agent_reward)\n",
    "#             action_dict[state_key][agent_first_move][\"count\"] += 1\n",
    "#         else:\n",
    "#             action_dict[state_key][agent_first_move] = {}\n",
    "#             action_dict[state_key][agent_first_move][\"avg_reward\"] = agent_reward\n",
    "#             action_dict[state_key][agent_first_move][\"count\"] = 1\n",
    "\n",
    "# a = (1, 2, 3)\n",
    "# type(a)\n",
    "# # combine two tuples\n",
    "# b = a + (4, 5, 6)\n",
    "# b\n",
    "# # append the value 7 to the tuple\n",
    "# b = b + (7,)\n",
    "# b\n",
    "\n",
    "\n",
    "def get_hp_array(game_state_agent, game_state_opp):\n",
    "    '''\n",
    "    '''\n",
    "    agent_pkm_hp_list = [game_state_agent.teams[0].active.hp]\n",
    "\n",
    "    for pkm in game_state_agent.teams[0].party:\n",
    "        agent_pkm_hp_list.append(pkm.hp)\n",
    "\n",
    "    opp_active_pkm_hp = game_state_opp.teams[0].active.hp\n",
    "\n",
    "    hp_array = np.array(agent_pkm_hp_list + [opp_active_pkm_hp])\n",
    "\n",
    "    return hp_array\n",
    "\n",
    "def get_hp_list(game_state_agent, game_state_opp):\n",
    "    '''\n",
    "    '''\n",
    "    agent_pkm_hp_list = [game_state_agent.teams[0].active.hp]\n",
    "\n",
    "    for pkm in game_state_agent.teams[0].party:\n",
    "        agent_pkm_hp_list.append(pkm.hp)\n",
    "\n",
    "    opp_active_pkm_hp = game_state_opp.teams[0].active.hp\n",
    "\n",
    "    hp_list = agent_pkm_hp_list + [opp_active_pkm_hp]\n",
    "\n",
    "    return hp_list\n",
    "\n",
    "def turn_game_state_into_dict_key(game_state_agent, game_state_opp,lookup_dict,\n",
    "    dmg_array,                      \n",
    "    pkm_hp_max = 480., dmg_scale_value = 600.):\n",
    "    '''\n",
    "    tuple is (\n",
    "        # HP\n",
    "        agent_active_pkm_hp, agent_party+_pkm_hp, opp_active_pkm_hp,\n",
    "        # DMG to opp\n",
    "        agent_active_pkm_dmg, agent_party_pkm_dmg,\n",
    "        # dmg from opp\n",
    "        # do this later\n",
    "        )\n",
    "    \n",
    "    If everything is on the scale of 0 to 100 picturing\n",
    "    8 buckets from 0 to 40 with increments of 5\n",
    "    6 buckets from 40 to 100 with increments of 10\n",
    "\n",
    "    preload a dict with the look up, then scal everything here\n",
    "\n",
    "    scaling dmg more than max hp so if move can do over 480 dmg has some sort of knowledge ofit\n",
    "    '''\n",
    "    # get arrays to make tuples out of for dict key\n",
    "    hp_array = get_hp_array(game_state_agent, game_state_opp)\n",
    "\n",
    "    hp_tuple = scale_hp_and_get_dict_value(hp_array, pkm_hp_max, lookup_dict)\n",
    "    dmg_tuple = scale_hp_and_get_dict_value(dmg_array, dmg_scale_value, lookup_dict)\n",
    "\n",
    "    dict_key = hp_tuple + dmg_tuple\n",
    "\n",
    "    return dict_key\n",
    "\n",
    "\n",
    "def scale_hp_and_get_dict_value(hp_array, max_hp, lookup_dict):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # scale by max hp then multiply by 100 to get into 0 to 99 range\n",
    "    hp_array = (hp_array / max_hp) * 100\n",
    "    # round, convert to int then clip to 0 to 99\n",
    "    hp_array = hp_array.round(0).astype(int).clip(0, 99)\n",
    "\n",
    "    \n",
    "\n",
    "    # for hp in hp_array:\n",
    "    #     hp_values_from_dict_tuple = hp_values_from_dict_tuple + (lookup_dict[hp],)\n",
    "\n",
    "    hp_values_from_dict_tuple = tuple(lookup_dict[hp] for hp in hp_array)\n",
    "\n",
    "    return hp_values_from_dict_tuple\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup_dict = make_lookup_dict()\n",
    "\n",
    "# a = turn_game_state_into_dict_key(game_state[0], game_state[1], lookup_dict, dmg_array)\n",
    "\n",
    "# print(a)\n",
    "\n",
    "# a = (1, 2, 3)\n",
    "# type(a)\n",
    "# # combine two tuples\n",
    "# b = a + (4, 5, 6)\n",
    "# b\n",
    "# print(b)\n",
    "# print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "# agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "# opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "# # set new environment with teams\n",
    "# env = PkmBattleEnv((agent_team, opp_team),\n",
    "#                 encode=(False, False)) \n",
    "\n",
    "# game_state, info = env.reset()\n",
    "\n",
    "# # for pkm in game_state[0].teams[1].party:\n",
    "# #     print(pkm.hp, pkm.fainted())\n",
    "# #     # if pkm.hp > 0.0 or not pkm.fainted():\n",
    "# #     #     is_more_than_opp_pkm_alive = True\n",
    "# #     #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<vgc.engine.PkmBattleEnv.PkmBattleEnv at 0x17e4a228ac0>,\n",
       " <vgc.engine.PkmBattleEnv.PkmBattleEnv at 0x17e4a2c5070>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train eval loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_statistical_action(action_dict, state_key):\n",
    "    '''\n",
    "    (8, 8, 6, 5, 4) {'attack': {'sum_wins': 89, 'count': 194}, 'swap': {'sum_wins': 52, 'count': 167}}\n",
    "    '''\n",
    "    best_action = 0\n",
    "    attack_key = 'attack'\n",
    "    swap_key = 'swap'\n",
    "    win_key = 'sum_wins'\n",
    "    count_key = 'count'\n",
    "\n",
    "    STOPPED HERE\n",
    "    # check if state_key in action_dict\n",
    "    # get the count and the wins\n",
    "    # plut them into the chi squared test\n",
    "\n",
    "    # if state_key in action_dict:\n",
    "    #     best_action = max(action_dict[state_key], key=action_dict[state_key].get)\n",
    "    # else:\n",
    "    #     best_action = 0\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_eval_loop(num_battles, is_eval, run_tag, is_save=True, action_dict_to_copy=None):\n",
    "    '''\n",
    "    STOPPED HERE NEED TO WORK IN THE EVAL CODE\n",
    "    then review logic then test\n",
    "    '''\n",
    "    if is_eval and action_dict_to_copy is not None:\n",
    "        action_lookup_dict = copy.deepcopy(action_dict_to_copy)\n",
    "\n",
    "    winner_dict = {\n",
    "        0:0,\n",
    "        1:0,\n",
    "        -1:0\n",
    "    }\n",
    "\n",
    "    pkm_env_action_dict = {\n",
    "        0:{},\n",
    "        1:{},\n",
    "    }\n",
    "\n",
    "    action_state_results_dict = {}\n",
    "\n",
    "    max_episode_steps = 250\n",
    "    agent_index = 0\n",
    "\n",
    "    lookup_dict = make_lookup_dict()\n",
    "    team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "    time_int = int(time.time())\n",
    "    save_tag =  f\"_{run_tag}_{time_int}\"\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    for battle_idx in range(num_battles):\n",
    "        \n",
    "        agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "        opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "        # set new environment with teams\n",
    "        env = PkmBattleEnv((agent_team, opp_team), encode=(False, False)) \n",
    "\n",
    "        game_state, info = env.reset()\n",
    "\n",
    "        is_first_move = True\n",
    "        agent_first_move = None\n",
    "        state_key = None\n",
    "\n",
    "        for episode_step in range(max_episode_steps):\n",
    "            if is_first_move:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    agent_pre_env_action = 0\n",
    "                    agent_first_move = 'attack'\n",
    "                else:\n",
    "                    agent_pre_env_action = 1\n",
    "                    agent_first_move = 'swap'\n",
    "                is_first_move = False\n",
    "            else:\n",
    "                agent_pre_env_action = 0\n",
    "\n",
    "            agent_env_action, agent_team_best_damage_list = turn_agent_action_into_env_action(agent_pre_env_action, game_state[0])\n",
    "            opp_action, opp_best_damage_list = get_best_active_damage_action(game_state[1])\n",
    "\n",
    "            if agent_pre_env_action == 1 and agent_env_action != 4:\n",
    "                print(\"Error agent action is 1 but env action is not 4 \")\n",
    "            elif agent_pre_env_action == 0:\n",
    "                if (agent_env_action < 0 or agent_env_action > 3):\n",
    "                    print(\"Error agent action is 0 but env action is not 0 to 3 \")\n",
    "                elif len(agent_team_best_damage_list) == 0:\n",
    "                    print(\"Error agent action is 0 but best damage list is empty\")\n",
    "                elif agent_team_best_damage_list[0] < 0:\n",
    "                    print(\"Error agent action is 0 but best damage is negative\")\n",
    "\n",
    "            if opp_action < 0 or opp_action > 3:\n",
    "                print(\"Error opp action is not 0 to 3\")\n",
    "            elif len(opp_best_damage_list) == 0:\n",
    "                print(\"Error opp best damage list is empty\")\n",
    "            elif opp_best_damage_list[0] < 0:\n",
    "                print(\"Error opp best damage is negative\")\n",
    "            \n",
    "            # get the state key\n",
    "            # only do this on the initial set up of the env\n",
    "            if state_key is None:\n",
    "                # for now just doing part dmg to opp\n",
    "                #dmg_array = np.array(agent_best_damage_list + [opp_best_damage,])\n",
    "                dmg_array = np.array(agent_team_best_damage_list)\n",
    "                state_key = turn_game_state_into_dict_key(game_state[0], game_state[1], lookup_dict, dmg_array)\n",
    "                if len(state_key) != 5:\n",
    "                    print(\"Error state key is not 5 long\")\n",
    "\n",
    "                if is_eval:\n",
    "                    # see if action look up dict says to do a different action\n",
    "                    if state_key in action_lookup_dict:\n",
    "                        # do logic for get if action is better than swap\n",
    "                        STOPPED HERE\n",
    "                        agent_pre_env_action = ...\n",
    "                        agent_env_action = turn_agent_action_into_env_action(agent_pre_env_action, game_state[0])\n",
    "\n",
    "                    pkm_env_action_dict = add_action_to_pkm_env_action_dict(agent_env_action, pkm_env_action_dict, 0)\n",
    "                    pkm_env_action_dict = add_action_to_pkm_env_action_dict(opp_action, pkm_env_action_dict, 1)\n",
    "\n",
    "            # enter action and step the env\n",
    "            action_list = [agent_env_action, opp_action]\n",
    "            game_state, _not_used_reward, terminated, truncated, info = env.step(action_list)  # for inference, we don't need reward\n",
    "\n",
    "            if episode_step == max_episode_steps - 1:\n",
    "                print('Warning: max steps reached')\n",
    "                terminated = True\n",
    "\n",
    "            if terminated:\n",
    "                winner = env.winner\n",
    "                if winner == agent_index:\n",
    "                    win_int = 1\n",
    "                else:\n",
    "                    win_int = 0\n",
    "\n",
    "                add_results_to_action_dict(action_state_results_dict, state_key, agent_first_move, win_int)\n",
    "\n",
    "                if winner in winner_dict:\n",
    "                    winner_dict[winner] += 1\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to run {(end_time - start_time) / 60:.3f} minutes\")\n",
    "    print(f\"Time to run {(end_time - start_time) / num_battles:.3f} seconds per battle\")\n",
    "    print(f\"Time to run {((end_time - start_time) / num_battles / 60 / 60) * 1000000:.3f} hours per million battles\")\n",
    "\n",
    "    print(winner_dict)\n",
    "\n",
    "    if is_save:\n",
    "        save_object_as_pkl(action_state_results_dict , f'action_dict_{save_tag}')\n",
    "        save_object_as_pkl(action_state_results_dict , f'action_dict_{winner_dict}')\n",
    "\n",
    "    return winner_dict, action_state_results_dict, pkm_env_action_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 0.001 minutes\n",
      "Time to run 0.005 seconds per battle\n",
      "Time to run 1.500 hours per million battles\n",
      "{0: 5, 1: 5, -1: 0}\n",
      "{0: 5, 1: 5, -1: 0}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_wd, test_ad, test_pead = build_train_eval_loop(10, is_eval=False, run_tag=\"test\", is_save=False )\n",
    "print(test_wd)\n",
    "print(len(test_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dict Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 73.454 minutes\n",
      "Time to run 0.004 seconds per battle\n",
      "Time to run 1.224 hours per million battles\n",
      "{0: 0, 1: 0, -1: 0}\n"
     ]
    }
   ],
   "source": [
    "num_battles = 1000000\n",
    "\n",
    "winner_dict = {\n",
    "    0:0,\n",
    "    1:0,\n",
    "    -1:0\n",
    "}\n",
    "\n",
    "pkm_env_action_dict = {\n",
    "    0:{},\n",
    "    1:{},\n",
    "}\n",
    "\n",
    "action_state_results_dict = {}\n",
    "\n",
    "max_episode_steps = 250\n",
    "agent_index = 0\n",
    "\n",
    "lookup_dict = make_lookup_dict()\n",
    "team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "time_int = int(time.time())\n",
    "save_tag =  f\"_smoke_test_{time_int}\"\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for battle_idx in range(num_battles):\n",
    "    \n",
    "    agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "    opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "    # set new environment with teams\n",
    "    env = PkmBattleEnv((agent_team, opp_team),\n",
    "                   encode=(False, False)) \n",
    "\n",
    "    game_state, info = env.reset()\n",
    "\n",
    "    is_first_move = True\n",
    "    agent_first_move = None\n",
    "    state_key = None\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        if is_first_move:\n",
    "            if np.random.rand() < 0.5:\n",
    "                agent_pre_env_action = 0\n",
    "                agent_first_move = 'attack'\n",
    "            else:\n",
    "                agent_pre_env_action = 1\n",
    "                agent_first_move = 'swap'\n",
    "            is_first_move = False\n",
    "        else:\n",
    "            agent_pre_env_action = 0\n",
    "\n",
    "        agent_env_action, agent_team_best_damage_list = turn_agent_action_into_env_action(agent_pre_env_action, game_state[0])\n",
    "        opp_action, opp_best_damage_list = get_best_active_damage_action(game_state[1])\n",
    "\n",
    "        if agent_pre_env_action == 1 and agent_env_action != 4:\n",
    "            print(\"Error agent action is 1 but env action is not 4 \")\n",
    "        elif agent_pre_env_action == 0:\n",
    "            if (agent_env_action < 0 or agent_env_action > 3):\n",
    "                print(\"Error agent action is 0 but env action is not 0 to 3 \")\n",
    "            elif len(agent_team_best_damage_list) == 0:\n",
    "                print(\"Error agent action is 0 but best damage list is empty\")\n",
    "            elif agent_team_best_damage_list[0] < 0:\n",
    "                print(\"Error agent action is 0 but best damage is negative\")\n",
    "\n",
    "        if opp_action < 0 or opp_action > 3:\n",
    "            print(\"Error opp action is not 0 to 3\")\n",
    "        elif len(opp_best_damage_list) == 0:\n",
    "            print(\"Error opp best damage list is empty\")\n",
    "        elif opp_best_damage_list[0] < 0:\n",
    "            print(\"Error opp best damage is negative\")\n",
    "        \n",
    "        # get the state key\n",
    "        # only do this on the initial set up of the env\n",
    "        if state_key is None:\n",
    "            # for now just doing part dmg to opp\n",
    "            #dmg_array = np.array(agent_best_damage_list + [opp_best_damage,])\n",
    "            dmg_array = np.array(agent_team_best_damage_list)\n",
    "            state_key = turn_game_state_into_dict_key(game_state[0], game_state[1], lookup_dict, dmg_array)\n",
    "            if len(state_key) != 5:\n",
    "                print(\"Error state key is not 5 long\")\n",
    "\n",
    "        # enter action and step the env\n",
    "        action_list = [agent_env_action, opp_action]\n",
    "        game_state, _not_used_reward, terminated, truncated, info = env.step(action_list)  # for inference, we don't need reward\n",
    "\n",
    "        if episode_step == max_episode_steps - 1:\n",
    "            print('Warning: max steps reached')\n",
    "            terminated = True\n",
    "\n",
    "        if terminated:\n",
    "            winner = env.winner\n",
    "            if winner == agent_index:\n",
    "                win_int = 1\n",
    "            else:\n",
    "                win_int = 0\n",
    "\n",
    "            add_results_to_action_dict(action_state_results_dict, state_key, agent_first_move, win_int)\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time to run {(end_time - start_time) / 60:.3f} minutes\")\n",
    "print(f\"Time to run {(end_time - start_time) / num_battles:.3f} seconds per battle\")\n",
    "print(f\"Time to run {((end_time - start_time) / num_battles / 60 / 60) * 1000000:.3f} hours per million battles\")\n",
    "\n",
    "print(winner_dict)\n",
    "# print(action_dict)\n",
    "\n",
    "\n",
    "save_object_as_pkl(action_state_results_dict , 'action_dict_smoke_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37543"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(action_state_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking action dict results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 6, 5, 4) {'attack': {'sum_wins': 89, 'count': 194}, 'swap': {'sum_wins': 52, 'count': 167}}\n"
     ]
    }
   ],
   "source": [
    "# for k, v in action_state_results_dict.items():\n",
    "#     print(k, v)\n",
    "#     break\n",
    "\n",
    "# tuple is (\n",
    "#     # HP\n",
    "#     agent_active_pkm_hp, agent_party+_pkm_hp, opp_active_pkm_hp,\n",
    "#     # DMG to opp\n",
    "#     agent_active_pkm_dmg, agent_party_pkm_dmg,\n",
    "#     # dmg from opp\n",
    "#     # do this later\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37543"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 11, 8, 5, 3) 0.082 0.135 61 52\n",
      "(8, 8, 9, 8, 9) 0.5 0.576 78 66\n",
      "(6, 6, 11, 8, 8) 0.528 0.604 72 96\n",
      "(5, 5, 11, 9, 9) 0.476 0.622 63 45\n",
      "(5, 8, 11, 9, 8) 0.441 0.535 59 71\n",
      "(5, 5, 11, 8, 8) 0.358 0.45 106 100\n",
      "(8, 5, 10, 8, 9) 0.44 0.508 50 65\n",
      "(8, 11, 8, 5, 5) 0.194 0.254 67 63\n",
      "(5, 5, 9, 8, 5) 0.312 0.365 64 52\n",
      "(6, 6, 8, 5, 11) 0.229 0.321 48 53\n",
      "(8, 11, 8, 5, 6) 0.355 0.462 62 52\n",
      "(8, 6, 11, 8, 8) 0.536 0.589 69 90\n",
      "(8, 5, 11, 9, 8) 0.456 0.518 57 56\n",
      "(6, 6, 5, 10, 9) 0.419 0.481 74 52\n",
      "(5, 11, 8, 7, 3) 0.23 0.283 61 46\n",
      "(8, 6, 10, 8, 9) 0.472 0.576 53 59\n",
      "count statistics 26.636 64.517 1 1144 6.0\n",
      "50 (5014,)\n",
      "75 (3380,)\n",
      "100 (2475,)\n",
      "150 (1425,)\n",
      "200 (883,)\n"
     ]
    }
   ],
   "source": [
    "as_copy = copy.deepcopy(action_state_results_dict)\n",
    "\n",
    "count_list = []\n",
    "\n",
    "for k, v in as_copy.items():\n",
    "    attack_count = v.get('attack', {}).get('count', 0)\n",
    "    swap_count = v.get('swap', {}).get('count', 0)\n",
    "    attack_wins = v.get('attack', {}).get('sum_wins', 0)\n",
    "    swap_wins = v.get('swap', {}).get('sum_wins', 0)\n",
    "\n",
    "    total_count = attack_count + swap_count\n",
    "    count_list.append(total_count)\n",
    "\n",
    "    if attack_count > 0:\n",
    "        attack_win_percent = attack_wins / attack_count\n",
    "    else:\n",
    "        attack_win_percent = None\n",
    "\n",
    "    if swap_count > 0:\n",
    "        swap_win_percent = swap_wins / swap_count\n",
    "    else:\n",
    "        swap_win_percent = None\n",
    "\n",
    "    if attack_win_percent is not None and swap_win_percent is not None and total_count > 100:\n",
    "        if swap_win_percent - attack_win_percent > 0.05:\n",
    "            print(k, np.round(attack_win_percent,3), np.round(swap_win_percent,3), attack_count, swap_count)\n",
    "\n",
    "    \n",
    "print(f\"count statistics {np.mean(count_list):.3f} {np.std(count_list):.3f} {np.min(count_list)} {np.max(count_list)} {np.median(count_list)}\")\n",
    "\n",
    "# find number of counts >= x\n",
    "count_array = np.array(count_list)\n",
    "for x in [50, 75, 100, 150, 200]:\n",
    "    print(x, count_array[count_array >= x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_state_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({8}, {10})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval Agents Loop\n",
    "* based on two agents and results, see how statistically signficant the difference is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 0.038 minutes\n",
      "Time to run 0.005 seconds per battle\n",
      "Time to run 1.281 hours per million battles\n",
      "{0: 236, 1: 264, -1: 0}\n",
      "{0: {3: 311, 1: 316, 0: 510, 2: 314}, 1: {0: 528, 3: 302, 1: 324, 2: 297}}\n"
     ]
    }
   ],
   "source": [
    "num_battles = 10\n",
    "\n",
    "winner_dict = {\n",
    "    0:0,\n",
    "    1:0,\n",
    "    -1:0\n",
    "}\n",
    "\n",
    "pkm_env_action_dict = {\n",
    "    0:{},\n",
    "    1:{},\n",
    "}\n",
    "\n",
    "action_lookup_dict = copy.deepcopy(action_state_results_dict)\n",
    "\n",
    "max_episode_steps = 250\n",
    "agent_index = 0\n",
    "\n",
    "lookup_dict = make_lookup_dict()\n",
    "team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "time_int = int(time.time())\n",
    "save_tag =  f\"_smoke_test_winner_dict_{time_int}\"\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for battle_idx in range(num_battles):\n",
    "    \n",
    "    agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "    opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "    # set new environment with teams\n",
    "    env = PkmBattleEnv((agent_team, opp_team),\n",
    "                   encode=(False, False)) \n",
    " \n",
    "    game_state, info = env.reset()\n",
    "\n",
    "    # is_first_move = True\n",
    "    # agent_first_move = None\n",
    "    state_key = None\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "\n",
    "        agent_pre_env_action = 0\n",
    "        agent_env_action, agent_team_best_damage_list = turn_agent_action_into_env_action(agent_pre_env_action, game_state[0])\n",
    "        opp_action, opp_best_damage_list = get_best_active_damage_action(game_state[1])\n",
    "\n",
    "        if opp_action < 0 or opp_action > 3:\n",
    "            print(\"Error opp action is not 0 to 3\")\n",
    "        elif len(opp_best_damage_list) == 0:\n",
    "            print(\"Error opp best damage list is empty\")\n",
    "        elif opp_best_damage_list[0] < 0:\n",
    "            print(\"Error opp best damage is negative\")\n",
    "            \n",
    "        if state_key is None:\n",
    "            # for now just doing part dmg to opp\n",
    "            #dmg_array = np.array(agent_best_damage_list + [opp_best_damage,])\n",
    "            dmg_array = np.array(agent_team_best_damage_list)\n",
    "            state_key = turn_game_state_into_dict_key(game_state[0], game_state[1], lookup_dict, dmg_array)\n",
    "\n",
    "            # for now only override the initial action, could change this to possibly swap later as well\n",
    "            if state_key in action_lookup_dict:\n",
    "                agent_env_action = max(action_lookup_dict[state_key], key=action_lookup_dict[state_key].get)\n",
    "\n",
    "\n",
    "        if agent_pre_env_action == 1 and agent_env_action != 4:\n",
    "            print(\"Error agent action is 1 but env action is not 4 \")\n",
    "        elif agent_pre_env_action == 0:\n",
    "            if (agent_env_action < 0 or agent_env_action > 3):\n",
    "                print(\"Error agent action is 0 but env action is not 0 to 3 \")\n",
    "            elif len(agent_team_best_damage_list) == 0:\n",
    "                print(\"Error agent action is 0 but best damage list is empty\")\n",
    "            elif agent_team_best_damage_list[0] < 0:\n",
    "                print(\"Error agent action is 0 but best damage is negative\")\n",
    "\n",
    "        \n",
    "   \n",
    "        # enter action and step the env\n",
    "        action_list = [agent_env_action, opp_action]\n",
    "\n",
    "        pkm_env_action_dict = add_action_to_pkm_env_action_dict(agent_env_action, pkm_env_action_dict, 0)\n",
    "        pkm_env_action_dict = add_action_to_pkm_env_action_dict(opp_action, pkm_env_action_dict, 1)\n",
    "\n",
    "        #print(action_list)\n",
    "        game_state, _not_used_reward, terminated, truncated, info = env.step(action_list)  # for inference, we don't need reward\n",
    "\n",
    "        if episode_step == max_episode_steps - 1:\n",
    "            print('Warning: max steps reached')\n",
    "            terminated = True\n",
    "\n",
    "        if terminated:\n",
    "            winner = env.winner\n",
    "            if winner == agent_index:\n",
    "                win_int = 1\n",
    "            else:\n",
    "                win_int = 0\n",
    "\n",
    "            if winner in winner_dict:\n",
    "                winner_dict[winner] += 1\n",
    "\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time to run {(end_time - start_time) / 60:.3f} minutes\")\n",
    "print(f\"Time to run {(end_time - start_time) / num_battles:.3f} seconds per battle\")\n",
    "print(f\"Time to run {((end_time - start_time) / num_battles / 60 / 60) * 1000000:.3f} hours per million battles\")\n",
    "\n",
    "print(winner_dict)\n",
    "print(pkm_env_action_dict)\n",
    "\n",
    "save_object_as_pkl(winner_dict, 'eval_winner_dict_smoke_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See if results significant\n",
    "* should probably do this with ELO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Win rate: 0.472\n",
      "Chi-square statistic: 2.916\n",
      "P-value: 0.08771\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(win_loss_draw1, win_loss_draw2):\n",
    "    \"\"\"\n",
    "    Performs a Chi-square test on two entities with win, loss, and draw counts.\n",
    "    \n",
    "    Parameters:\n",
    "    - win_loss_draw1: A list or tuple of win, loss, and draw counts for entity 1 (e.g., [wins, losses, draws]).\n",
    "    - win_loss_draw2: A list or tuple of win, loss, and draw counts for entity 2.\n",
    "    \n",
    "    Returns:\n",
    "    - Chi-square statistic, p-value, and interpretation as a string.\n",
    "    \"\"\"\n",
    "    # Create a contingency table\n",
    "    contingency_table = [win_loss_draw1, win_loss_draw2]\n",
    "    \n",
    "    # Perform the Chi-square test\n",
    "    chi2, pval, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Interpret the results\n",
    "    interpretation = (\"There is a statistically significant difference in outcomes between the two entities.\"\n",
    "                      if pval < 0.05 else\n",
    "                      \"There is no statistically significant difference in outcomes between the two entities.\")\n",
    "    \n",
    "    print(f'Player 1 Win rate: { win_loss_draw1[0] / sum(win_loss_draw1):.3f}')\n",
    "\n",
    "\n",
    "    print(f'Chi-square statistic: {chi2:.3f}')\n",
    "    print(f'P-value: {pval:.5f}')\n",
    "\n",
    "    return chi2, pval, interpretation\n",
    "\n",
    "# # Example usage\n",
    "#chi2, pval, interpretation = chi_square_test([120, 80, 50], [130, 70, 60])\n",
    "\n",
    "chi2, pval, interpretation = chi_square_test([winner_dict[0],\n",
    "                                              winner_dict[1],\n",
    "                                              #winner_dict[-1]\n",
    "                                              ],\n",
    "                                             [winner_dict[1],\n",
    "                                              winner_dict[0],\n",
    "                                              #winner_dict[-1]\n",
    "                                              ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- x is  150\n",
      "Player 1 Win rate: 0.500\n",
      "Chi-square statistic: 0.000\n",
      "P-value: 1.00000\n",
      "________\n",
      "--- x is  155\n",
      "Player 1 Win rate: 0.517\n",
      "Chi-square statistic: 0.540\n",
      "P-value: 0.46243\n",
      "________\n",
      "--- x is  160\n",
      "Player 1 Win rate: 0.533\n",
      "Chi-square statistic: 2.407\n",
      "P-value: 0.12082\n",
      "________\n",
      "--- x is  165\n",
      "Player 1 Win rate: 0.550\n",
      "Chi-square statistic: 5.607\n",
      "P-value: 0.01789\n",
      "________\n",
      "--- x is  170\n",
      "Player 1 Win rate: 0.567\n",
      "Chi-square statistic: 10.140\n",
      "P-value: 0.00145\n",
      "________\n",
      "--- x is  175\n",
      "Player 1 Win rate: 0.583\n",
      "Chi-square statistic: 16.007\n",
      "P-value: 0.00006\n",
      "________\n",
      "--- x is  180\n",
      "Player 1 Win rate: 0.600\n",
      "Chi-square statistic: 23.207\n",
      "P-value: 0.00000\n",
      "________\n",
      "--- x is  185\n",
      "Player 1 Win rate: 0.617\n",
      "Chi-square statistic: 31.740\n",
      "P-value: 0.00000\n",
      "________\n",
      "--- x is  190\n",
      "Player 1 Win rate: 0.633\n",
      "Chi-square statistic: 41.607\n",
      "P-value: 0.00000\n",
      "________\n",
      "--- x is  195\n",
      "Player 1 Win rate: 0.650\n",
      "Chi-square statistic: 52.807\n",
      "P-value: 0.00000\n",
      "________\n"
     ]
    }
   ],
   "source": [
    "# test where it is significant\n",
    "\n",
    "for x in range(150, 200, 5):\n",
    "    winner_dict = {\n",
    "        0:x,\n",
    "        1:300-x,\n",
    "        -1:0\n",
    "    }\n",
    "    print(\"--- x is \", x)\n",
    "    chi2, pval, interpretation = chi_square_test([winner_dict[0],\n",
    "                                                winner_dict[1],\n",
    "                                                #winner_dict[-1]\n",
    "                                                ],\n",
    "                                                [winner_dict[1],\n",
    "                                                winner_dict[0],\n",
    "                                                #winner_dict[-1]\n",
    "                                                ],)\n",
    "    print(\"________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rates: 0.540 vs. 0.460\n",
      "T-statistic: 1.96261\n",
      "P-value: 0.05015\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "def compare_rates(successes_group1, observations_group1, successes_group2, observations_group2):\n",
    "    \"\"\"\n",
    "    Compares two observed rates using a two-sample t-test and prints the t-statistic and p-value.\n",
    "    \n",
    "    Parameters:\n",
    "    - successes_group1: Number of successes in group 1\n",
    "    - observations_group1: Number of observations in group 1\n",
    "    - successes_group2: Number of successes in group 2\n",
    "    - observations_group2: Number of observations in group 2\n",
    "    \"\"\"\n",
    "    # Calculate rates\n",
    "    rate_group1 = successes_group1 / observations_group1\n",
    "    rate_group2 = successes_group2 / observations_group2\n",
    "\n",
    "    # Convert rates to \"success\" arrays\n",
    "    data_group1 = np.array([1] * successes_group1 + [0] * (observations_group1 - successes_group1))\n",
    "    data_group2 = np.array([1] * successes_group2 + [0] * (observations_group2 - successes_group2))\n",
    "\n",
    "    # Perform the two-sample t-test\n",
    "    stat, pval = ttest_ind(data_group1, data_group2)\n",
    "\n",
    "    # Print rounded t-statistic and p-value\n",
    "    print(\"Rates: {:.3f} vs. {:.3f}\".format(rate_group1, rate_group2))\n",
    "    print(f'T-statistic: {stat:.5f}')\n",
    "    print(f'P-value: {pval:.5f}')\n",
    "\n",
    "# Example usage\n",
    "#compare_rates(45, 100, 50, 120)\n",
    "\n",
    "compare_rates(winner_dict[0], num_battles, winner_dict[1], num_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Rates: 0.500 vs. 0.500\n",
      "T-statistic: 0.00000\n",
      "P-value: 1.00000\n",
      "160\n",
      "Rates: 0.533 vs. 0.467\n",
      "T-statistic: 1.63390\n",
      "P-value: 0.10281\n",
      "170\n",
      "Rates: 0.567 vs. 0.433\n",
      "T-statistic: 3.28991\n",
      "P-value: 0.00106\n",
      "175\n",
      "Rates: 0.583 vs. 0.417\n",
      "T-statistic: 4.13349\n",
      "P-value: 0.00004\n",
      "200\n",
      "Rates: 0.667 vs. 0.333\n",
      "T-statistic: 8.64581\n",
      "P-value: 0.00000\n",
      "225\n",
      "Rates: 0.750 vs. 0.250\n",
      "T-statistic: 14.11855\n",
      "P-value: 0.00000\n",
      "250\n",
      "Rates: 0.833 vs. 0.167\n",
      "T-statistic: 21.87236\n",
      "P-value: 0.00000\n",
      "275\n",
      "Rates: 0.917 vs. 0.083\n",
      "T-statistic: 36.86585\n",
      "P-value: 0.00000\n"
     ]
    }
   ],
   "source": [
    "for test_wins in [150, 160, 170, 175, 200, 225, 250, 275]:\n",
    "    print(test_wins)\n",
    "    compare_rates(test_wins, num_battles, num_battles-test_wins, num_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(episode_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<vgc.engine.PkmBattleEnv.PkmBattleEnv at 0x17e31fafac0>,\n",
       "  <vgc.engine.PkmBattleEnv.PkmBattleEnv at 0x17e31fa12e0>),\n",
       " [1.0, 1.0],\n",
       " True,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.88888888888889"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".266 / 60 * 1000000 /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dict size with raw values\n",
    "* so with 5m iters, like 4.7m different dict values\n",
    "* so some grouping needs to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 10956.973 seconds\n",
      "4750104\n"
     ]
    }
   ],
   "source": [
    "test_iters = 5000000\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "raw_stats_dict = {}\n",
    "team_generator = RandomTeamGenerator(2)\n",
    "\n",
    "for i in range(test_iters):\n",
    "    agent_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "    opp_team = team_generator.get_team().get_battle_team([0, 1, ])\n",
    "\n",
    "    game_state, info = env.reset()\n",
    "\n",
    "    env = PkmBattleEnv((agent_team, opp_team), encode=(False, False)) \n",
    "\n",
    "    game_state, info = env.reset()\n",
    "\n",
    "    _agent_env_action, agent_team_best_damage_list = turn_agent_action_into_env_action(0, game_state[0])\n",
    "    _opp_action, opp_best_damage_list = get_best_active_damage_action(game_state[1])\n",
    "\n",
    "    hp_list = get_hp_list(game_state[0], game_state[1])\n",
    "    #print(hp_list, agent_team_best_damage_list, opp_best_damage_list)\n",
    "\n",
    "    state_tuple = tuple(hp_list + agent_team_best_damage_list + opp_best_damage_list)\n",
    "    #print(state_tuple)\n",
    "\n",
    "    if state_tuple in raw_stats_dict:\n",
    "        raw_stats_dict[state_tuple] += 1\n",
    "    else:\n",
    "        raw_stats_dict[state_tuple] = 1\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"Time to run {time_end - time_start:.3f} seconds\")\n",
    "\n",
    "print(len(raw_stats_dict))\n",
    "\n",
    "save_object_as_pkl(raw_stats_dict, 'raw_state_dict_smoke_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.043603611111111"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10956.973 /60 /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hp_act_dict = copy.deepcopy(raw_stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the distribution of HP values and dmg values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336.0, 120.0, 120.0, 102.0, 282.0, 630.0, 261.0) 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in raw_hp_act_dict.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{336.0: 484424, 120.0: 3779517, 156.0: 3378332, 264.0: 1144966, 192.0: 2453402, 228.0: 1692512, 408.0: 164303, 300.0: 758680, 372.0: 296296, 480.0: 22651, 444.0: 75229}\n",
      "{102.0: 602589, 282.0: 644930, 630.0: 186926, 261.0: 947623, 423.0: 284529, 204.0: 680716, 132.0: 315649, 153.0: 904877, 414.0: 382101, 315.0: 693529, 99.0: 237773, 354.0: 175684, 210.0: 1395427, 138.0: 1062649, 211.5: 85819, 207.0: 1101337, 492.0: 287177, 564.0: 175764, 348.0: 629654, 198.0: 368290, 174.0: 1386176, 276.0: 798295, 66.0: 172952, 246.0: 978168, 369.0: 463244, 141.0: 153193, 420.0: 441586, 306.0: 491293, 130.5: 136611, 390.0: 53556, 318.0: 374454, 159.0: 104128, 105.0: 198419, 238.5: 48117, 522.0: 278020, 123.0: 165291, 738.0: 120375, 184.5: 125389, 103.5: 137683, 585.0: 21952, 846.0: 73532, 265.5: 22306, 477.0: 158584, 195.0: 16064, 87.0: 86796, 60.0: 31486, 90.0: 57314, 177.0: 51219, 636.0: 96930, 76.5: 66736, 531.0: 73829, 954.0: 40382, 45.0: 30880, 708.0: 44751, 157.5: 167336, 780.0: 13565, 1062.0: 18638, 30.0: 12627, 51.0: 27591, 69.0: 63996, 49.5: 11328, 1170.0: 5628, 292.5: 6595, 33.0: 8382, 0.0: 216, 22.5: 1014, 15.0: 746}\n"
     ]
    }
   ],
   "source": [
    "# first 3 are HP, next four are dmg\n",
    "hp_dict_count = {}\n",
    "dmg_dict_count = {}\n",
    "hp_list = []\n",
    "dmg_list = []\n",
    "\n",
    "for k, _ in raw_hp_act_dict.items():\n",
    "    hp_key = k[:3]\n",
    "    dmg_key = k[3:]\n",
    "\n",
    "    for hp_value in hp_key:\n",
    "        if hp_value in hp_dict_count:\n",
    "            hp_dict_count[hp_value] += 1\n",
    "        else:\n",
    "            hp_dict_count[hp_value] = 1\n",
    "        hp_list.append(hp_value)\n",
    "\n",
    "    for dmg_value in dmg_key:\n",
    "        if dmg_value in dmg_dict_count:\n",
    "            dmg_dict_count[dmg_value] += 1\n",
    "        else:\n",
    "            dmg_dict_count[dmg_value] = 1\n",
    "        dmg_list.append(dmg_value)\n",
    "    \n",
    "\n",
    "print(hp_dict_count)\n",
    "print(dmg_dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### notes\n",
    "* so very few HP values\n",
    "* also not that many low dmg values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{120.0: 3779517,\n",
      " 156.0: 3378332,\n",
      " 192.0: 2453402,\n",
      " 228.0: 1692512,\n",
      " 264.0: 1144966,\n",
      " 300.0: 758680,\n",
      " 336.0: 484424,\n",
      " 372.0: 296296,\n",
      " 408.0: 164303,\n",
      " 444.0: 75229,\n",
      " 480.0: 22651}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(hp_dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "potential HP dict keys\n",
    "400 and over\n",
    "320 to 400\n",
    "240 to 320\n",
    "160 to 240\n",
    "80 to 160\n",
    "0 to 80\n",
    "\n",
    "\n",
    "dmg values\n",
    "0 to 80\n",
    "80 to 160\n",
    "160 to 240\n",
    "240 to 320\n",
    "320 to 400\n",
    "400 to 480\n",
    "480 to 560\n",
    "560 +\n",
    "\n",
    "\n",
    "5x5 would be like\n",
    "0 to 96\n",
    "96 to 192\n",
    "192 to 288\n",
    "288 to 384\n",
    "384 to 480\n",
    "\n",
    "then for dmg values could do like\n",
    "0 to 120\n",
    "120 to 240\n",
    "240 to 360\n",
    "360 to 480\n",
    "480 +\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "480 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884736\n",
      "884736\n",
      "9765625\n",
      "1048576\n"
     ]
    }
   ],
   "source": [
    "print(6**3 * 8**4)\n",
    "print(6*6*6*8*8*8*8)\n",
    "\n",
    "print(5**4 * 5**6)\n",
    "\n",
    "print(4**4 * 4**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884736\n",
      "884736\n",
      "3 3 2187\n",
      "3 4 6912\n",
      "3 5 16875\n",
      "3 6 34992\n",
      "3 7 64827\n",
      "3 8 110592\n",
      "3 9 177147\n",
      "4 3 5184\n",
      "4 4 16384\n",
      "4 5 40000\n",
      "4 6 82944\n",
      "4 7 153664\n",
      "4 8 262144\n",
      "4 9 419904\n",
      "5 3 10125\n",
      "5 4 32000\n",
      "5 5 78125\n",
      "5 6 162000\n",
      "5 7 300125\n",
      "5 8 512000\n",
      "5 9 820125\n",
      "6 3 17496\n",
      "6 4 55296\n",
      "6 5 135000\n",
      "6 6 279936\n",
      "6 7 518616\n",
      "6 8 884736\n",
      "6 9 1417176\n",
      "7 3 27783\n",
      "7 4 87808\n",
      "7 5 214375\n",
      "7 6 444528\n",
      "7 7 823543\n",
      "7 8 1404928\n",
      "7 9 2250423\n",
      "8 3 41472\n",
      "8 4 131072\n",
      "8 5 320000\n",
      "8 6 663552\n",
      "8 7 1229312\n",
      "8 8 2097152\n",
      "8 9 3359232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for a in range(3,9):\n",
    "    for b in range(3,10):\n",
    "        print(a, b, (a**3)*(b**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 216,\n",
      " 15.0: 746,\n",
      " 22.5: 1014,\n",
      " 30.0: 12627,\n",
      " 33.0: 8382,\n",
      " 45.0: 30880,\n",
      " 49.5: 11328,\n",
      " 51.0: 27591,\n",
      " 60.0: 31486,\n",
      " 66.0: 172952,\n",
      " 69.0: 63996,\n",
      " 76.5: 66736,\n",
      " 87.0: 86796,\n",
      " 90.0: 57314,\n",
      " 99.0: 237773,\n",
      " 102.0: 602589,\n",
      " 103.5: 137683,\n",
      " 105.0: 198419,\n",
      " 123.0: 165291,\n",
      " 130.5: 136611,\n",
      " 132.0: 315649,\n",
      " 138.0: 1062649,\n",
      " 141.0: 153193,\n",
      " 153.0: 904877,\n",
      " 157.5: 167336,\n",
      " 159.0: 104128,\n",
      " 174.0: 1386176,\n",
      " 177.0: 51219,\n",
      " 184.5: 125389,\n",
      " 195.0: 16064,\n",
      " 198.0: 368290,\n",
      " 204.0: 680716,\n",
      " 207.0: 1101337,\n",
      " 210.0: 1395427,\n",
      " 211.5: 85819,\n",
      " 238.5: 48117,\n",
      " 246.0: 978168,\n",
      " 261.0: 947623,\n",
      " 265.5: 22306,\n",
      " 276.0: 798295,\n",
      " 282.0: 644930,\n",
      " 292.5: 6595,\n",
      " 306.0: 491293,\n",
      " 315.0: 693529,\n",
      " 318.0: 374454,\n",
      " 348.0: 629654,\n",
      " 354.0: 175684,\n",
      " 369.0: 463244,\n",
      " 390.0: 53556,\n",
      " 414.0: 382101,\n",
      " 420.0: 441586,\n",
      " 423.0: 284529,\n",
      " 477.0: 158584,\n",
      " 492.0: 287177,\n",
      " 522.0: 278020,\n",
      " 531.0: 73829,\n",
      " 564.0: 175764,\n",
      " 585.0: 21952,\n",
      " 630.0: 186926,\n",
      " 636.0: 96930,\n",
      " 708.0: 44751,\n",
      " 738.0: 120375,\n",
      " 780.0: 13565,\n",
      " 846.0: 73532,\n",
      " 954.0: 40382,\n",
      " 1062.0: 18638,\n",
      " 1170.0: 5628}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dmg_dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 67\n",
      "20152452\n"
     ]
    }
   ],
   "source": [
    "print(len(hp_dict_count), len(dmg_dict_count))\n",
    "print(11*11*11 * 67*67*67*67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740195513856780056217081019504370\n"
     ]
    }
   ],
   "source": [
    "print(11**6 + 67**18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336.0, 120.0, 120.0)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can then put into pandas and see the distribution of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
